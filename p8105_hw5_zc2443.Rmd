---
title: "p8105_hw5_zc2443"
output: github_document
author: "Ziyang Chen"
date: 11-3-2019
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rvest)
```

# Problem 1 
```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))

replace_missing = function(vec) {
  if (is.numeric(vec)) {
    vec = replace(vec, is.na(vec), round(mean(vec, na.rm = TRUE), digits = 1))
    #vec(is.na(vec)) = mean(vec, na.rm = TRUE)
  }
  else if (is.character(vec)) {
    vec = replace(vec, is.na(vec), "virginica")
    #vec(is.na(vec)) = "virginica"
  }
  vec
}

map(iris_with_missing, replace_missing)
```

# Problem 2

```{r}
tibble(list.files("./data")) %>% 
  rename(file_name = `list.files("./data")`) %>%
  mutate(tran = "./data/", tran2 = str_c(tran, file_name) ,data = map(tran2, read.csv)) %>% 
  unnest() %>% 
  mutate(file_name = str_replace(file_name, "\\_", " ")) %>% 
  mutate(file_name = str_replace(file_name, ".csv", "")) %>% 
  rename(arm_id = file_name) %>% 
  select(-tran, -tran2) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "value") %>% 
  mutate(value = round(value, digits = 2)) %>% 
  ggplot(aes(x = week, y = value, group = arm_id, color = arm_id)) +
  geom_path() + 
  labs(caption = "Observations on each subject over time",
       x = "Week",
       y = "Observations") +
  viridis::scale_color_viridis(discrete = TRUE)
```

From the spaghetti plot we can see that observations for subjects in experimental arm are increasing over time. However, the observations for subjects in control arm are fluctuating around some fix values over time.

# Problem 3

```{r}
set.seed(1)

sim_regression = function(n = 30, beta0 = 2, beta1 = 0) {
  
  sim_data = tibble(
    x = rnorm(n, mean = 0, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, sqrt(50))
  )
  
  ls_fit = lm(y ~ x, data = sim_data)
  
  tibble(
    beta1_hat = coef(ls_fit)[2],
    p_value = broom::tidy(ls_fit)[[2,5]] #extract the p-value
  )
}

sim_regression()

beta1_0 = rerun(10000, sim_regression(30, 2, 0)) %>%
  bind_rows() %>% 
  mutate(beta = 0)

beta1_1 = rerun(10000, sim_regression(30, 2, 1)) %>%
  bind_rows() %>% 
  mutate(beta = 1)

beta1_2 = rerun(10000, sim_regression(30, 2, 2)) %>%
  bind_rows() %>% 
  mutate(beta = 2)

beta1_3 = rerun(10000, sim_regression(30, 2, 3)) %>%
  bind_rows() %>% 
  mutate(beta = 3)

beta1_4 = rerun(10000, sim_regression(30, 2, 4)) %>%
  bind_rows() %>% 
  mutate(beta = 4)

beta1_5 = rerun(10000, sim_regression(30, 2, 5)) %>%
  bind_rows() %>% 
  mutate(beta = 5)

beta1_6 = rerun(10000, sim_regression(30, 2, 6)) %>%
  bind_rows() %>% 
  mutate(beta = 6)

bind_rows(beta1_0, beta1_1, beta1_2, beta1_3, beta1_4, beta1_5, beta1_6) %>% 
  mutate(reject = case_when(
    p_value < 0.05 ~ "1",
    TRUE ~ "0"
  )) %>% 
  group_by(beta) %>% 
  summarize(reject_sum = sum(as.numeric(reject)), n_obs = n()) %>% 
  mutate(proportion = round((reject_sum/n_obs), digits = 2)) %>% 
  ggplot(aes(x = beta, y = proportion)) +
  geom_bar(stat = "identity")
```

From the bar plot we can see that the larger the effect size, the larger the power. Hence, as the effect size increases, we have greater probability to reject the false null hypothesis.

```{r}
reject_data = bind_rows(beta1_0, beta1_1, beta1_2, beta1_3, beta1_4, beta1_5, beta1_6) %>% 
  filter(p_value < 0.05) %>% 
  group_by(beta) %>% 
  summarize(avg_beta1 = mean(beta1_hat))
  
bind_rows(beta1_0, beta1_1, beta1_2, beta1_3, beta1_4, beta1_5, beta1_6) %>% 
  group_by(beta) %>% 
  summarize(avg_beta1 = mean(beta1_hat)) %>% 
  ggplot(aes(x = beta, y = avg_beta1)) + 
  geom_point() +
  geom_point(data = reject_data, color = "blue") +
  labs(x = "True Beta", y = "Estimated Beta") +
  scale_y_continuous(breaks = seq(0,6.5,0.3)) +
  scale_x_continuous(breaks = seq(0,6,1)) +
  theme_bw()
```

The sample average of beta1 across tests for which the null is rejected does not approximately equal to the true value of beta1. From the last question we know that as long as the effect size gets larger, we are more likely to have p-value that rejects the null. Hence, those beta1's with p-vlaue less than 0.05 are tend to be large and they are not good estimate of the true beta1.